{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect-Based Sentiment Analysis: Findings from Natural Language\n",
    "#### Code File \\#2: Baseline Model\n",
    "\n",
    "Tahmeed Tureen - University of Michigan, Ann Arbor<br>\n",
    "Python file: <b>data-engin-tureen.ipynb</b> <br>\n",
    "Description: Code that implements the SemEval baseline model for 2014 Task 4 as discussed in the paper (Pontiki et al.; 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up relevant libraries\n",
    "import math\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3156, 7)\n"
     ]
    }
   ],
   "source": [
    "# Read in Data\n",
    "train_data = pickle.load(open(\"pickled_data/pickled_train_data.pkl\", \"rb\"))\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(557, 7)\n"
     ]
    }
   ],
   "source": [
    "test_data = pickle.load(open(\"pickled_data/pickled_test_data.pkl\", \"rb\"))\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1025, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semEval_test_data = pickle.load(open(\"pickled_data/semEval_TestData.pkl\", \"rb\"))\n",
    "semEval_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models\n",
    "The 2014 SemEval 2014 committee proposed the following baseline models:\n",
    "\n",
    "- **Aspect Term Extraction**: In training, create a \"dictionary\" (word bank) that will consist of all of the aspect terms that show up in the Training Data. In test, go through each review and pick out aspect terms that are in this word back\n",
    "\n",
    "- **Aspect Term Polarity**: In training, make a dictionary where the key is the aspect term and value is the sentiment that is the most frequent associated with that term. In test, assign the most frequent sentiment of the aspect term in the training data to the test term.\n",
    "\n",
    "- **Aspect Category Extraction**: In testing, compare the current review with all of the reviews in the training data and whicherver training review has the highest dice coefficient with the current review, we assign the test review with that category\n",
    "\n",
    "- **Aspect Category Polarity**: In training, make a dictionary where the key is the Category and value is the sentiment that is the most frequent associated with that term. In test, assign the most frequent sentiment of the Category in the training data to the test Category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Some Comments**: These are very naive and funky baseline models, but we use them as our lowest threshold metric and we want our built models to essentially beat these guys!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics\n",
    "\n",
    "Here we define the F1 Score, Precision and Recall for **Term Extraction** as discussed in (Pontiki et al.; 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F1_SemEval(predictions, truth):\n",
    "    # need to calculate precision, recall\n",
    "    intersect_SnG = 0 # intersection of extractions and truths\n",
    "    cap_S = 0.0 # set of extractions\n",
    "    cap_G = 0.0 # set of truths\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        current_pred = predictions[i]\n",
    "        current_truth = truth[i]\n",
    "#         print(current_pred)\n",
    "#         print(current_truth)\n",
    "        \n",
    "        # numerator for both precision and recall (number of terms in prediction that is also in the truth)\n",
    "        intersect_SnG += len([term for term in current_pred if term in current_truth])\n",
    "        cap_S += len(current_pred)\n",
    "        cap_G += len(current_truth)\n",
    "#         print(\"SnG\", intersect_SnG)\n",
    "#         print(\"S:\", cap_S)\n",
    "#         print(\"G\", cap_G)\n",
    "        \n",
    "    # After loop is over we can now calculate the Precision and Recall values\n",
    "    prec = float(intersect_SnG) / float(cap_S)\n",
    "    recall = float(intersect_SnG) / float(cap_G)\n",
    "    \n",
    "    f1_score = ( 2.0 * prec * recall ) / (prec + recall)    \n",
    "    return f1_score, prec, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Aspect Term Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "We use our training data (n = 3167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1124 unique aspect terms in our training corpus\n"
     ]
    }
   ],
   "source": [
    "# Create the Naive word bank\n",
    "train_aspTerms_bank = []\n",
    "\n",
    "for term in train_data.Aspect_Term:\n",
    "    train_aspTerms_bank = train_aspTerms_bank + term\n",
    "\n",
    "# Get rid of repeated words (not necessary, but good practice to do this)\n",
    "train_aspTerms_bank = set(train_aspTerms_bank)\n",
    "print(\"We have\", len(train_aspTerms_bank), \"unique aspect terms in our training corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "- On sibling test split (n = 557)\n",
    "- On SemEval annotated test data (n = 1025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load up SemEval's stopwords\n",
    "# Stopwords, imported from NLTK (v 2.0.4)\n",
    "stopwords = set(\n",
    "    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "     'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
    "     'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',\n",
    "     'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "     'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "     'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "     'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n",
    "     'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, go through the sibling test set's reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aspect_terms_extraction = []\n",
    "for rev in test_data.Review:\n",
    "    tokenized_rev = rev.split() # basic tokenization by whitespace\n",
    "    current_rev_terms = [] # create container for the current review's extracted terms\n",
    "    \n",
    "    # Go through each token in the review then extract if match\n",
    "    for token in tokenized_rev:\n",
    "        if token in train_aspTerms_bank and token not in stopwords:\n",
    "            current_rev_terms.append(token)\n",
    "            \n",
    "    aspect_terms_extraction.append(current_rev_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go trough the SemEval annotated test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aspterm_extr_SemEval = []\n",
    "\n",
    "for rev in semEval_test_data.Review:\n",
    "    \n",
    "    tokenized_rev = rev.split() # basic tokenization by whitespace\n",
    "    current_rev_terms = [] # container for the current review's extracted terms\n",
    "    \n",
    "    # Go through each token\n",
    "    for token in tokenized_rev:\n",
    "        if token in train_aspTerms_bank and token not in stopwords:\n",
    "            current_rev_terms.append(token)\n",
    "            \n",
    "    aspterm_extr_SemEval.append(current_rev_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Calculate the raw correct/total score and the F1 Scores for both sets of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw correct prop on Sibling Test Set: 0.34470377019748655\n",
      "F1 Score, Precision, and Recall on Sibling Test Set: (0.5375647668393783, 0.5563002680965148, 0.5200501253132832)\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw correct prop on Sibling Test Set:\", sum(aspect_terms_extraction == test_data.Aspect_Term) / test_data.shape[0])\n",
    "print(\"F1 Score, Precision, and Recall on Sibling Test Set:\", \\\n",
    "      F1_SemEval(aspect_terms_extraction, list(test_data.Aspect_Term)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw correct prop on SemEval Annotated Test Set: 0.34146341463414637\n",
      "F1 Score, Precision, and Recall on SemEval Test Set: (0.508235294117647, 0.5788667687595712, 0.4529658478130617)\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw correct prop on SemEval Annotated Test Set:\", \\\n",
    "      sum(aspterm_extr_SemEval == semEval_test_data.Aspect_Term) / semEval_test_data.shape[0])\n",
    "print(\"F1 Score, Precision, and Recall on SemEval Test Set:\", \\\n",
    "      F1_SemEval(predictions = aspterm_extr_SemEval, truth = list(semEval_test_data.Aspect_Term)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- We note that baseline performs poorly in terms of F1 Scores on both sets of test data. Scores are just above 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Aspect Term Polarity\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aspTerm_PolClassifier = defaultdict(lambda : {'positive' : 0, 'negative' : 0, 'neutral' : 0})\n",
    "\n",
    "for rev in train_data.itertuples():\n",
    "    # loop through all of the terms in the Aspect Terms list in  a review\n",
    "    for index in range(len(rev.Aspect_Term)):\n",
    "        \n",
    "        aspTerm_PolClassifier[rev.Aspect_Term[index]][rev.Aspect_Polarity[index]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1124"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aspTerm_PolClassifier.keys()) # should be 1124\n",
    "# max(stats, key=stats.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "- Testing on the extracted aspect terms from Task 1\n",
    "- Testing on the ground truth test terms from the sibling test dataset (Assuming all terms were extracted correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Testing on extracted data\n",
    "baseline_aspTerm_polarities = []\n",
    "\n",
    "for terms in aspect_terms_extraction:\n",
    "    current_review = []\n",
    "    \n",
    "    for term in terms:\n",
    "        \n",
    "        curr_polarity = max(aspTerm_PolClassifier[term], key = aspTerm_PolClassifier[term].get)\n",
    "        current_review.append(curr_polarity)\n",
    "        \n",
    "    baseline_aspTerm_polarities.append(current_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32854578096947934\n"
     ]
    }
   ],
   "source": [
    "print(sum(baseline_aspTerm_polarities == test_data.Aspect_Polarity) / 557 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing on truth aspect terms in the test data\n",
    "baseline_aspTerm_polarities_2 = []\n",
    "\n",
    "for terms in list(test_data.Aspect_Term):\n",
    "    current_review = []\n",
    "    \n",
    "    for term in terms:\n",
    "        curr_polarity = max(aspTerm_PolClassifier[term], key = aspTerm_PolClassifier[term].get)\n",
    "        current_review.append(curr_polarity)\n",
    "        \n",
    "    baseline_aspTerm_polarities_2.append(current_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6104129263913824\n"
     ]
    }
   ],
   "source": [
    "print(sum(baseline_aspTerm_polarities_2 == test_data.Aspect_Polarity) / 557)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: \n",
    "\n",
    "- When we run the classifier on our extracted terms, the test accuracy is approximately 32.9%. This makes sense because we also performed poorly on the baseline extraction in Task 1. This performance is conditional on the performance of the previous task.\n",
    "\n",
    "- When we run the classifier on the test data and go through the extracted terms there, the accuracy performance rises to 61%, which is obviously an improvement but not a great score. (**Note:**) This performance is NOT conditional on Task 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Aspect-Category Detection\n",
    "\n",
    "\"For every test sentence s, the k most similar to s training sentences are retrieved (as in the SB2 baseline). Then, s is assigned the m most frequent aspect category labels of the k retrieved sentences;\n",
    "m is the most frequent number of aspect category\n",
    "labels per sentence among the k sentences.\" - (Pontiki et al.; 2014)\n",
    "\n",
    "**Interpretation:** Essentially, what they mean by this is to use the Dice Coefficient to get similarity ratings for a test sentence and all of the training sentences. Then pick the highest rating and assign the category label that is associated with that training sentence to the current test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a dice coefficient function (strucutred after SemEval's code)\n",
    "# Dice coefficient\n",
    "def dice_coeff(train_sen, test_sen, stopwords_in = stopwords):\n",
    "    tokenize = lambda t: set([w for w in t.split() if (w not in stopwords)]) # define tokenize fxn\n",
    "    train_sen = tokenize(train_sen)\n",
    "    test_sen = tokenize(test_sen)\n",
    "    \n",
    "#     print(train_sen)\n",
    "#     print(test_sen)\n",
    "#     print(train_sen.intersection(test_sen))\n",
    "    \n",
    "    dice_val = 2.0 * len(train_sen.intersection(test_sen)) / (len(train_sen) + len(test_sen))\n",
    "    return dice_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll go ahead and extract the categories for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aspCategory_Extraction = []\n",
    "aspCategory_PolarityTest = [] # also doing Task 4\n",
    "\n",
    "for test_rev in test_data.Review:\n",
    "    \n",
    "    category_label = \"other\" # category label to assign\n",
    "    category_polarity = \"neutral\" # category polarity to assign \n",
    "    max_diceCoeff = 0.0 # container for max dice coefficient\n",
    "    \n",
    "    for train_rev in train_data.itertuples():\n",
    "        \n",
    "        curr_diceCoeff = dice_coeff(train_sen= train_rev.Review, test_sen= test_rev)\n",
    "        \n",
    "        if curr_diceCoeff > max_diceCoeff:\n",
    "            category_label = train_rev.Category\n",
    "            category_polarity = train_rev.Category_Polarity\n",
    "    \n",
    "    aspCategory_Extraction.append(category_label)\n",
    "    aspCategory_PolarityTest.append(category_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5502680965147453, 0.5486134313397929, 0.5519327731092437)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_SemEval(predictions=aspCategory_Extraction, truth= list(test_data.Category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'food', 'food', 'service', 'service']\n",
      "['ambience', 'food', 'food', 'food', 'service']\n",
      "0.4254937163375224\n"
     ]
    }
   ],
   "source": [
    "print(aspCategory_Extraction[0:5])\n",
    "print(list(test_data.Category)[0:5])\n",
    "print(sum(aspCategory_Extraction == test_data.Category) / 557)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 Score associated with the Category classification/extraction is approximately 55% on our sibling test data and the raw accuracy is 42%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Aspect Category Polarity Classification\n",
    "\n",
    "Same schema as Task 2\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspCategory_PolClassifier = defaultdict(lambda : {'positive' : 0, 'negative' : 0, 'neutral' : 0})\n",
    "\n",
    "for review in train_data.itertuples():\n",
    "    aspCategory_PolClassifier[review.Category][review.Category_Polarity] += 1\n",
    "\n",
    "len(aspCategory_PolClassifier.keys()) # should be five!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {'ambience': {'negative': 78, 'neutral': 61, 'positive': 228},\n",
       "             'food': {'negative': 182, 'neutral': 125, 'positive': 743},\n",
       "             'other': {'negative': 176, 'neutral': 309, 'positive': 470},\n",
       "             'price': {'negative': 100, 'neutral': 23, 'positive': 154},\n",
       "             'service': {'negative': 179, 'neutral': 46, 'positive': 282}})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspCategory_PolClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aspCategory_PolarityTest2 = []\n",
    "\n",
    "for cat in test_data.Category:\n",
    "    aspCategory_PolarityTest2.append(max(aspCategory_PolClassifier[cat], key = aspCategory_PolClassifier[cat].get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557\n",
      "557\n"
     ]
    }
   ],
   "source": [
    "print(len(aspCategory_PolarityTest))\n",
    "print(len(aspCategory_PolarityTest2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4524236983842011\n"
     ]
    }
   ],
   "source": [
    "print(sum(aspCategory_PolarityTest == test_data.Category_Polarity) / 557)\n",
    "print(sum(aspCategory_PolarityTest == test_data.Category_Polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
