{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect-Based Sentiment Analysis: Findings from Natural Language\n",
    "#### Code File \\#3: Implementations for our Proposed Models\n",
    "\n",
    "Tahmeed Tureen - University of Michigan, Ann Arbor<br>\n",
    "Python file: <b>asba-proposed-model-implementations.ipynb</b> <br>\n",
    "Description: Code that implements the our proposed models/algorithms for 2014 Task 4 (Pontiki et al.; 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up relevant libraries\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3156, 7)\n",
      "(557, 7)\n",
      "(1025, 4)\n"
     ]
    }
   ],
   "source": [
    "# Read in Data\n",
    "train_data = pickle.load(open(\"pickled_data/pickled_train_data.pkl\", \"rb\"))\n",
    "print(train_data.shape)\n",
    "test_data = pickle.load(open(\"pickled_data/pickled_test_data.pkl\", \"rb\"))\n",
    "print(test_data.shape)\n",
    "semEval_test_data = pickle.load(open(\"pickled_data/semEval_TestData.pkl\", \"rb\"))\n",
    "print(semEval_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase A: Tasks 1\n",
    "We will first do aspect term (subtask 1) and aspect category classification (subtask 3) before we perform sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up spaCy for POS Tagging and Dependency Parsing\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_spacy = spacy.load(\"en_core_web_md\") # load up spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(\n",
    "    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "     'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
    "     'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',\n",
    "     'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "     'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "     'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "     'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n",
    "     'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test if my_spacy works first\n",
    "random_sen_spacy = my_spacy(u\"The chinese pasta was great, but the waiter, Jason, was rude! I love Lucy, the tv show\")\n",
    "\n",
    "# list(random_sen_spacy.ents)\n",
    "# list(random_sen_spacy.sents)\n",
    "# for word in random_sen_spacy:\n",
    "#     print(word.text, word.pos_, word.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the F1 score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F1_SemEval(predictions, truth):\n",
    "    # need to calculate precision, recall\n",
    "    intersect_SnG = 0 # intersection of extractions and truths\n",
    "    cap_S = 0.0 # set of extractions\n",
    "    cap_G = 0.0 # set of truths\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        current_pred = predictions[i]\n",
    "        current_truth = truth[i]\n",
    "#         print(current_pred)\n",
    "#         print(current_truth)\n",
    "        \n",
    "        # numerator for both precision and recall (number of terms in prediction that is also in the truth)\n",
    "        intersect_SnG += len([term for term in current_pred if term in current_truth])\n",
    "        cap_S += len(current_pred)\n",
    "        cap_G += len(current_truth)\n",
    "#         print(\"SnG\", intersect_SnG)\n",
    "#         print(\"S:\", cap_S)\n",
    "#         print(\"G\", cap_G)\n",
    "        \n",
    "    # After loop is over we can now calculate the Precision and Recall values\n",
    "    prec = float(intersect_SnG) / float(cap_S)\n",
    "    recall = float(intersect_SnG) / float(cap_G)\n",
    "    \n",
    "    f1_score = ( 2.0 * prec * recall ) / (prec + recall)    \n",
    "    return f1_score, prec, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 Approach:\n",
    "There is no model training associated with this method\n",
    "\n",
    "- Create a word bank of all of the aspect terms in the training data (of all lower cases)\n",
    "- Part-Of-Speech (POS) tag the reviews\n",
    "- Use the POS tagger to only extract nouns\n",
    "- filter out all nouns that do not belong in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3171                       [Lassi]\n",
       "3172                      [coffee]\n",
       "3173                            []\n",
       "3174    [wine list, food, service]\n",
       "3175    [wine list, food, service]\n",
       "Name: Aspect_Term, dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_aspTerms = [] # make a vector of the ground truth aspect terms but lower cased\n",
    "\n",
    "for terms in test_data.Aspect_Term:\n",
    "    curr_review = []\n",
    "    \n",
    "    for term in terms:\n",
    "        curr_review.append(term.lower())\n",
    "        \n",
    "    truth_aspTerms.append(curr_review)\n",
    "\n",
    "# print(truth_aspTerms[15:20])\n",
    "test_data.Aspect_Term[15:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercases\n",
    "Convert everything to lower case, let's see if this helps us improve our F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the word bank\n",
    "train_aspTermsBank = []\n",
    "for terms in train_data.Aspect_Term:\n",
    "    for term in terms:\n",
    "        train_aspTermsBank.append(term.lower()) # convert to lower case\n",
    "        \n",
    "train_aspTermsBank = set(train_aspTermsBank)\n",
    "\n",
    "len(train_aspTermsBank) # goes down to 1059 when we convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see how much performance goes up/down when we just do this\n",
    "aspTerms_test = []\n",
    "for review in test_data.Review:\n",
    "    review = review.split()\n",
    "    current_rev = []\n",
    "    \n",
    "    for term in review:\n",
    "        term = term.lower()\n",
    "        \n",
    "        if term in train_aspTermsBank:\n",
    "            current_rev.append(term)\n",
    "    aspTerms_test.append(current_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5477707006369427, 0.5569948186528497, 0.5388471177944862)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_SemEval(predictions=aspTerms_test, truth=truth_aspTerms) # goes up by only 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorporate POS tags for extraction\n",
    "\n",
    "Use the POS tagger from the spaCy library and strip n-gram Noun objects. Let's see how well this does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's incorporate spaCy\n",
    "aspTerms_test = []\n",
    "\n",
    "for review in test_data.Review:\n",
    "    asp_TERMS = []\n",
    "    \n",
    "    curr_review = review.lower()\n",
    "    curr_review = my_spacy(curr_review) # convert to spaCy object\n",
    "    \n",
    "    for n_chunk in curr_review.noun_chunks:\n",
    "        asp_term = ''\n",
    "        for obj in n_chunk:\n",
    "            # If not a noun, then skip... we aren't interested\n",
    "            if (obj.pos_ != \"NOUN\"):\n",
    "                continue\n",
    "            if (obj.pos_ == \"NOUN\"): # we interested\n",
    "                asp_term += obj.text + \" \" # add white space incase we have a compound\n",
    "        \n",
    "        asp_term = asp_term[:-1] # drop the hanging white space at the end\n",
    "        \n",
    "        if (len(asp_term) > 0):\n",
    "            asp_TERMS.append(asp_term) # append the extracted aspect term to the list of extractions\n",
    "        \n",
    "    aspTerms_test.append(asp_TERMS) # append to the full extraction list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['evening', 'evening', 'friends'],\n",
       " ['lamb meat'],\n",
       " ['pad thai'],\n",
       " ['time', 'food quality', 'service', 'part', 'reason'],\n",
       " ['time', 'food quality', 'service', 'part', 'reason']]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspTerms_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['lamb meat'],\n",
       " ['pad thai'],\n",
       " ['food quality', 'service'],\n",
       " ['food quality', 'service']]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_aspTerms[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5777777777777778, 0.4476584022038568, 0.8145363408521303)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_SemEval(aspTerms_test, truth_aspTerms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recall shoots up immensely and our F1 score goes up by approx. 5% than the baseline! It looks like our precision goes down and this may be a consequence of extracting too many words... nouns that don't even belong to any of the aspect categories!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorporate the trained aspect term words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's incorporate spaCy\n",
    "aspTerms_test = []\n",
    "\n",
    "for review in test_data.Review:\n",
    "    asp_TERMS = []\n",
    "    \n",
    "    curr_review = review.lower()\n",
    "    curr_review = my_spacy(curr_review) # convert to spaCy object\n",
    "    \n",
    "    for n_chunk in curr_review.noun_chunks:\n",
    "        asp_term = ''\n",
    "        for obj in n_chunk:\n",
    "            # If not a noun, then skip... we aren't interested\n",
    "            if (obj.pos_ != \"NOUN\"):\n",
    "                continue\n",
    "            if (obj.pos_ == \"NOUN\"): # we interested\n",
    "                asp_term += obj.text + \" \" # add white space incase we have a compound\n",
    "        \n",
    "        asp_term = asp_term[:-1] # drop the hanging white space at the end\n",
    "        \n",
    "        if (len(asp_term) > 0) and asp_term in train_aspTermsBank: \n",
    "            asp_TERMS.append(asp_term) # append the extracted aspect term to the list of extractions\n",
    "        \n",
    "    aspTerms_test.append(asp_TERMS) # append to the full extraction list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.746971736204576, 0.8066860465116279, 0.6954887218045113)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_SemEval(aspTerms_test, truth_aspTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pizza', 'service'], ['pizza', 'service'], ['waiter', 'wine'], ['oil'], ['place', 'food']]\n",
      "[['pizza', 'service'], ['pizza', 'service'], ['waiter', 'red wine', 'hot tea', 'outside'], ['oil'], ['place', 'food']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(aspTerms_test[5:10])\n",
    "print(truth_aspTerms[5:10])\n",
    "\"hot tea\" in train_aspTermsBank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dang! The F1 Score goes up to 74%... this is great but we can probably do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorporate word similarities using pre-trained Word Embeddings from Wikipedia\n",
    "\n",
    "The word embeddings were provided by the SI 630: NLP Instructional Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim # for reading in Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up word2vec word embeddings from wikipedia\n",
    "# word2vec_wiki = gensim.models.KeyedVectors.load_word2vec_format('wiki.word2vec.min-100.bin',\n",
    "#                                                                 binary = False, unicode_errors = 'ignore')\n",
    "# Pickle this file\n",
    "# pickle.dump(word2vec_wiki, open(\"pickled_data/pickled_wordEmbeddings_wiki.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load up Pickled word embeddings\n",
    "word2vec_wiki = pickle.load(open(\"pickled_data/pickled_wordEmbeddings_wiki.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85072654\n",
      "0.9314064\n",
      "0.92584974\n",
      "0.87186205\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_wiki.n_similarity(\"pizza\", \"food\"))\n",
    "print(word2vec_wiki.n_similarity(\"waiter\", \"service\"))\n",
    "print(word2vec_wiki.n_similarity(\"music\", \"ambience\"))\n",
    "print(word2vec_wiki.n_similarity(\"bill\", \"price\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna make a simple empirical decision and say that any aspect term that has a word similarity greater than 0.85 with any of the aspect categories can be included into the aspect term extraction... let's see if this helps us improve our F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's incorporate spaCy\n",
    "aspTerms_test = []\n",
    "sim_threshold = 0.95\n",
    "\n",
    "for review in test_data.Review:\n",
    "    asp_TERMS = []\n",
    "    \n",
    "    curr_review = review.lower()\n",
    "    curr_review = my_spacy(curr_review) # convert to spaCy object\n",
    "    \n",
    "    for n_chunk in curr_review.noun_chunks:\n",
    "        asp_term = ''\n",
    "        for obj in n_chunk:\n",
    "            # If not a noun, then skip... we aren't interested\n",
    "            if (obj.pos_ != \"NOUN\"):\n",
    "                continue\n",
    "            if (obj.pos_ == \"NOUN\"): # we interested\n",
    "                asp_term += obj.text + \" \" # add white space incase we have a compound\n",
    "        \n",
    "        asp_term = asp_term[:-1] # drop the hanging white space at the end\n",
    "        \n",
    "        if (len(asp_term) > 0) and asp_term in train_aspTermsBank: \n",
    "            asp_TERMS.append(asp_term) # append the extracted aspect term to the list of extractions\n",
    "            \n",
    "        elif (len(asp_term) > 0):\n",
    "            asp_term_no_WS = asp_term.replace(\" \", \"_\") # replace white space with underscore o.w. n_similarity won't work\n",
    "            \n",
    "            if word2vec_wiki.n_similarity(\"food\", asp_term_no_WS) > sim_threshold:\n",
    "                asp_TERMS.append(asp_term)\n",
    "            \n",
    "            elif word2vec_wiki.n_similarity(\"service\", asp_term_no_WS) > sim_threshold:\n",
    "                asp_TERMS.append(asp_term)\n",
    "            \n",
    "            elif word2vec_wiki.n_similarity(\"ambience\", asp_term_no_WS) > sim_threshold:\n",
    "                asp_TERMS.append(asp_term)\n",
    "            \n",
    "            elif word2vec_wiki.n_similarity(\"price\", asp_term_no_WS) > sim_threshold:\n",
    "                asp_TERMS.append(asp_term)\n",
    "                \n",
    "    aspTerms_test.append(asp_TERMS) # append to the full extraction list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6893716970052847, 0.6486187845303868, 0.7355889724310777) threshold: 0.98\n"
     ]
    }
   ],
   "source": [
    "print(F1_SemEval(aspTerms_test, truth_aspTerms), \"threshold: 0.98\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['evening', 'evening', 'friends'], [], ['pad thai'], ['food quality', 'service'], ['food quality', 'service']]\n",
      "[[], ['lamb meat'], ['pad thai'], ['food quality', 'service'], ['food quality', 'service']]\n"
     ]
    }
   ],
   "source": [
    "print(aspTerms_test[0:5])\n",
    "print(truth_aspTerms[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the word similarities feature doesn't seem to help too much... we played around with the similarity thresholds... and we haven't beaten the previous score..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Steps\n",
    "\n",
    "For now, we stop here with this task... there are multiple ways we can go about to improve the score\n",
    "\n",
    "- Better Word Embeddings\n",
    "- Dependency Parsing"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
